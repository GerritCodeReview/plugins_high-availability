{
  "comments": [
    {
      "key": {
        "uuid": "69793d48_32f6a58c",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 7,
      "author": {
        "id": 1012987
      },
      "writtenOn": "2018-08-15T19:36:09Z",
      "side": 1,
      "message": "I am not convinced that this is a good idea.\n\nLimiting the rate can cause to have stale value in the caches of the passive node.",
      "range": {
        "startLine": 7,
        "startChar": 0,
        "endLine": 7,
        "endChar": 46
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e13ae62a_ec831175",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 7,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2018-08-15T22:32:42Z",
      "side": 1,
      "message": "The passive node is not active and thus it is unlikely to receive traffic.\nIn case of a catastrophic failure on the active node, the passive will start getting traffic but it may have lost some calls, because the active node is queueing anyway and the alignment of the failover node is async/best-effort.\n\nSending 1000 req/sec to invalidate the same cache for the same key, or just 10 req/sec could improve the alignment of the failover node because would ease the congestions of the master-to-failover node, avoiding the redundant calls.\n\nAt the end of the day, do we really care about invalidating the same cache key 1000 times a second on a failover node? What\u0027s the rationale behind it? If a cache entry has been invalidated 1 msec ago, because the node is passive anyway, it will still be empty the msec afterward.\n\nWhat makes you think that reducing the rate of invalidation (1000 req/sec to 10 req/sec for instance) would cause stale caches?",
      "parentUuid": "69793d48_32f6a58c",
      "range": {
        "startLine": 7,
        "startChar": 0,
        "endLine": 7,
        "endChar": 46
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "73aa2ec6_cc154ebf",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 7,
      "author": {
        "id": 1012987
      },
      "writtenOn": "2018-08-16T15:04:04Z",
      "side": 1,
      "message": "\u003e The passive node is not active and thus it is unlikely to receive traffic.\n\nNot for us. We toggle traffic between nodes quite often for various reasons(e.g. change gerrit and restart Gerrit, Deploy new version of Gerrit, take active node offline for investigating issues, ...). We even have a whole monitoring system that detect performance degradation and will automatically make passive the active one.\n\nWe can almost say that we use both nodes in active/active mode. That is why we absolutely need to \"passive\" node to always be up to date, we do not want a best effort thing.\n\n\u003e What makes you think that reducing the rate of invalidation (1000 req/sec to 10 req/sec for instance) would cause stale caches?\n\nI do not see in the current implementation throttle the cache invalidation of the same key, you globally throttle the invalidation. Reducing the rate of invalidation might cause some entries to never be invalidated if they always happen to not be in the ones going through.",
      "parentUuid": "e13ae62a_ec831175",
      "range": {
        "startLine": 7,
        "startChar": 0,
        "endLine": 7,
        "endChar": 46
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6aba6408_ae8b3327",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 7,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2018-08-17T05:56:38Z",
      "side": 1,
      "message": "\u003e \u003e The passive node is not active and thus it is unlikely to receive traffic.\n\u003e \n\u003e Not for us. We toggle traffic between nodes quite often for various reasons(e.g. change gerrit and restart Gerrit, Deploy new version of Gerrit, take active node offline for investigating issues, ...). We even have a whole monitoring system that detect performance degradation and will automatically make passive the active one.\n\nThat\u0027s interesting, can you share some of it?\nI was actually thinking about leveraging the Gerrit Analytics engine to predict decisions on how to use effectively the balancing between nodes, based on traffic trends, either historical or real-time.\n\n\u003e We can almost say that we use both nodes in active/active mode. That is why we absolutely need to \"passive\" node to always be up to date, we do not want a best effort thing.\n\nYes, in that scenario I believe you are right. This change would not make any sense in your use-case.\nWould then make sense to use a shared index (ElasticSearch) and a shared cache (Redis)?\n\nThe current mechanism of aligning cache evictions and reindex has always a latency and is never real-time. The implementation relies on a queue and worker threads.\n\nIn theory, Gerrit should not say \"success\" to an operation if the shared index and cache have not been updated, so that the real-time alignment is guaranteed.\n\n\u003e \u003e What makes you think that reducing the rate of invalidation (1000 req/sec to 10 req/sec for instance) would cause stale caches?\n\u003e \n\u003e I do not see in the current implementation throttle the cache invalidation of the same key, you globally throttle the invalidation.\n\nTrue, this change is, at the current stage, both RFC and WIP. My intention is to throttle on keys and skip multiple invalidations of the same one within the throttle period.\n\n\u003e Reducing the rate of invalidation might cause some entries to never be invalidated if they always happen to not be in the ones going through.\n\nNope if you throttle on the same key. Invalidating the same key 1000 times/sec or 10 times/sec has exactly the same result.",
      "parentUuid": "73aa2ec6_cc154ebf",
      "range": {
        "startLine": 7,
        "startChar": 0,
        "endLine": 7,
        "endChar": 46
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "fae5303f_00a49005",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 9,
      "author": {
        "id": 1012987
      },
      "writtenOn": "2018-08-15T19:36:09Z",
      "side": 1,
      "message": "this is a bit strong, cache evictions is limited by size of the thread pool and those HTTP calls are small and quite fast to execute, I do not think this is killing the passive node.",
      "range": {
        "startLine": 9,
        "startChar": 7,
        "endLine": 9,
        "endChar": 11
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "f65f8ced_17953ec0",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 9,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2018-08-15T22:32:42Z",
      "side": 1,
      "message": "Let me rephrase: kill the failover node or, when you are limiting on the active node, cause congestion on the sending side :-)",
      "parentUuid": "fae5303f_00a49005",
      "range": {
        "startLine": 9,
        "startChar": 7,
        "endLine": 9,
        "endChar": 11
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ff86bd6d_5d1c2b7f",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 14,
      "author": {
        "id": 1012987
      },
      "writtenOn": "2018-08-15T19:36:09Z",
      "side": 1,
      "message": "The problem is the forwarded indexing and stream events are causing the entries to be reloaded in the caches so I think we should keep every cache evictions, even if we have redundant ones. The goal of HA plugin is maintain passive node up to date to be able to failover without user noticing. If we go down the path proposed by this change, we will end up with a passive that could be not up to date and this is wrong.",
      "range": {
        "startLine": 13,
        "startChar": 9,
        "endLine": 14,
        "endChar": 41
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9a74ffb7_965bd911",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 14,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2018-08-15T22:32:42Z",
      "side": 1,
      "message": "\u003e The problem is the forwarded indexing and stream events are causing the entries to be reloaded in the caches \n\nI did not see the cache reload mechanism: can you point me in the right direction? AFAIK the cache invalidation just removes the entry and doesn\u0027t reload it.\n\n\u003e so I think we should keep every cache evictions, even if we have redundant ones. The goal of HA plugin is maintain passive node up to date to be able to failover without user noticing.\n\nAgreed, but that isn\u0027t working atm. The two nodes are not always aligned and having too much congestion on the active node would slow down the alignment.\n\n\u003e If we go down the path proposed by this change, we will end up with a passive that could be not up to date and this is wrong.\n\nI believe it would be exactly the opposite, based on my tests.",
      "parentUuid": "ff86bd6d_5d1c2b7f",
      "range": {
        "startLine": 13,
        "startChar": 9,
        "endLine": 14,
        "endChar": 41
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c9f0e5ed_698d2e24",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 14,
      "author": {
        "id": 1012987
      },
      "writtenOn": "2018-08-16T15:04:04Z",
      "side": 1,
      "message": "\u003e \u003e The problem is the forwarded indexing and stream events are causing the entries to be reloaded in the caches \n\u003e \n\u003e I did not see the cache reload mechanism: can you point me in the right direction? AFAIK the cache invalidation just removes the entry and doesn\u0027t reload it.\n\nWhat I meant is passive is loading values in the caches all the time even if it does not receive user facing traffic, it does receive traffic from HA plugin forwarding. For example, projects are loaded in the cache when stream-events is reinjected on the passive node to evaluate visibility.",
      "parentUuid": "9a74ffb7_965bd911",
      "range": {
        "startLine": 13,
        "startChar": 9,
        "endLine": 14,
        "endChar": 41
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6dbcd6bb_046cae26",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 14,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2018-08-17T05:56:38Z",
      "side": 1,
      "message": "\u003e \u003e \u003e The problem is the forwarded indexing and stream events are causing the entries to be reloaded in the caches \n\u003e \u003e \n\u003e \u003e I did not see the cache reload mechanism: can you point me in the right direction? AFAIK the cache invalidation just removes the entry and doesn\u0027t reload it.\n\u003e \n\u003e What I meant is passive is loading values in the caches all the time even if it does not receive user facing traffic, it does receive traffic from HA plugin forwarding. For example, projects are loaded in the cache when stream-events is reinjected on the passive node to evaluate visibility.\n\nGotcha, not directly then but indirectly by other operations.\nHowever, again, I do not see the value of having all the intermediate values in the msecs or microsec intervals if, at the end of the day, the cached entry ends up with the same value after the sequence of intervals.\n\nBear in mind that:\n- Cache invalidation events are sent queued with retries and in parallel\n- Cache, Index and Stream events are not globally synchronized\n\nThat means that you could miss anyway the \"perfect\" synchronization of the cache invalidation and the stream events anyway. If you want them to be perfectly synchronized, then the overall mechanism needs to change and this plugin cease to fulfil that scenario.",
      "parentUuid": "c9f0e5ed_698d2e24",
      "range": {
        "startLine": 13,
        "startChar": 9,
        "endLine": 14,
        "endChar": 41
      },
      "revId": "f1ff1a767c77dccab526acbe68d67b30f13ac1ab",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    }
  ]
}